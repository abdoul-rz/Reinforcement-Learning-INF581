{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91eWH3ZiMtxH"
   },
   "source": [
    "# VAE for MNIST digits generation\n",
    "\n",
    "The goal of this notebook is to present some recent works dealing with variational auto-encoder (VAE). It is based on code from a <a href=\"https://dataflowr.github.io/website/\">course of Marc Lelarge</a>, which itself borrows from existing classical implementations of VAEs in PyTorch that can be found [here](https://github.com/pytorch/examples/tree/master/vae) or [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65).\n",
    "\n",
    "\n",
    "\n",
    "We will use MNIST dataset and a basic VAE architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onVZXPp6MtxP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    \n",
    "def plot_reconstruction(model, n=24):\n",
    "    x,_ = next(iter(data_loader))\n",
    "    x = x[:n,:,:,:].to(device)\n",
    "    try:\n",
    "        out, _, _, log_p = model(x.view(-1, image_size)) \n",
    "    except:\n",
    "        out, _, _ = model(x.view(-1, image_size)) \n",
    "    x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
    "    out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
    "    show(out_grid)\n",
    "\n",
    "def plot_generation(model, n=24):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n, z_dim).to(device)\n",
    "        out = model.decode(z).view(-1, 1, 28, 28)\n",
    "\n",
    "    out_grid = torchvision.utils.make_grid(out).cpu()\n",
    "    show(out_grid)\n",
    "\n",
    "def plot_conditional_generation(model, n=8, z_dim=2, fix_number=None):\n",
    "    with torch.no_grad():\n",
    "        matrix = np.zeros((n,n_classes))\n",
    "        matrix[:,0] = 1\n",
    "\n",
    "        if fix_number is None:\n",
    "            final = matrix[:]\n",
    "            for i in range(1,n_classes):\n",
    "                final = np.vstack((final,np.roll(matrix,i)))\n",
    "            z = torch.randn(8*n_classes, z_dim).to(device)\n",
    "            y_onehot = torch.tensor(final).type(torch.FloatTensor).to(device)\n",
    "            concat_input = torch.cat([z, y_onehot], 1)\n",
    "            out = model.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
    "        else:\n",
    "            z = torch.randn(n, z_dim).to(device)\n",
    "            y_onehot = torch.tensor(np.roll(matrix, fix_number)).type(torch.FloatTensor).to(device)\n",
    "            out = model.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
    "\n",
    "    out_grid = torchvision.utils.make_grid(out).cpu()\n",
    "    show(out_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ic647cGkMtxb"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a directory if not exists\n",
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pgwvL5SvMtxm"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "data_dir = 'data'\n",
    "# MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root=data_dir,\n",
    "                                     train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UESJllvCMtxu"
   },
   "source": [
    "## Brief reminder on variational autoencoders\n",
    "\n",
    "Consider a latent variable model with a data variable $x\\in \\mathcal{X}$ and a latent variable $z\\in \\mathcal{Z}$, $p(z,x) = p(z)p_\\theta(x|z)$. Given the data $x_1,\\dots, x_n$, we want to train the model by maximizing the marginal log-likelihood:\n",
    "\\begin{eqnarray*}\n",
    "\\mathcal{L} = \\mathbf{E}_{p_d(x)}\\left[\\log p_\\theta(x)\\right]=\\mathbf{E}_{p_d(x)}\\left[\\log \\int_{\\mathcal{Z}}p_{\\theta}(x|z)p(z)dz\\right],\n",
    "  \\end{eqnarray*}\n",
    "  where $p_d$ denotes the empirical distribution of $X$: $p_d(x) =\\frac{1}{n}\\sum_{i=1}^n \\delta_{x_i}(x)$.\n",
    "\n",
    " To avoid the (often) difficult computation of the integral above, the idea behind variational methods is to instead maximize a lower bound to the log-likelihood:\n",
    "  \\begin{eqnarray*}\n",
    "\\mathcal{L} \\geq L(p_\\theta(x|z),q(z|x)) =\\mathbf{E}_{p_d(x)}\\left[\\mathbf{E}_{q(z|x)}\\left[\\log p_\\theta(x|z)\\right]-\\mathrm{KL}\\left( q(z|x)||p(z)\\right)\\right].\n",
    "  \\end{eqnarray*}\n",
    "  Any choice of $q(z|x)$ gives a valid lower bound. Variational autoencoders replace the variational posterior $q(z|x)$ by an inference network $q_{\\phi}(z|x)$ that is trained together with $p_{\\theta}(x|z)$ to jointly maximize $L(p_\\theta,q_\\phi)$.\n",
    "  \n",
    "The variational posterior $q_{\\phi}(z|x)$ is also called the **encoder** and the generative model $p_{\\theta}(x|z)$, the **decoder** or generator.\n",
    "\n",
    "The first term $\\mathbf{E}_{q(z|x)}\\left[\\log p_\\theta(x|z)\\right]$ is the negative reconstruction error. Indeed under a gaussian assumption i.e. $p_{\\theta}(x|z) = \\mathcal{N}(\\mu_{\\theta}(z), I)$ the term $\\log p_\\theta(x|z)$ reduces to $\\propto \\|x-\\mu_\\theta(z)\\|^2$, which is often used in practice. The term $\\mathrm{KL}\\left( q(z|x)||p(z)\\right)$ can be seen as a regularization term, where the variational posterior $q_\\phi(z|x)$ should be matched to the prior $p(z)= \\mathcal{N}(0, I)$.\n",
    "\n",
    "Variational Autoencoders were introduced by [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114), see also [(Doersch, 2016)](https://arxiv.org/abs/1606.05908) for a tutorial.\n",
    "\n",
    "There are various examples of VAE in PyTorch available [here](https://github.com/pytorch/examples/tree/master/vae) or [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65). The code below is taken from this last source.\n",
    "\n",
    "![A variational autoencoder.](vae.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TWtQVnsAMtxw"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return torch.sigmoid(self.fc5(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2klHMA7TMtx2"
   },
   "source": [
    "Here for the loss, instead of MSE for the reconstruction loss, we take Binary Cross-Entropy. The code below is still from the PyTorch tutorial (with minor modifications to avoid warnings!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hs3Wnd8Mtx4"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        \n",
    "        # Compute reconstruction loss and kl divergence\n",
    "        # For KL divergence between Gaussians, see Appendix B in VAE paper or (Doersch, 2016):\n",
    "        # https://arxiv.org/abs/1606.05908\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size, kl_div.item()/batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XinKgE7AMtx-"
   },
   "source": [
    "Let see how our network reconstructs our last batch. We display pairs of original digits and reconstructed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLTFvi0SMtyA"
   },
   "outputs": [],
   "source": [
    "plot_reconstruction(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZC7nauVyMtyF"
   },
   "source": [
    "Let's see now how our network generates new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehuLHz4NMtyH"
   },
   "outputs": [],
   "source": [
    "plot_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hddb3q43MtyN"
   },
   "source": [
    "Not great, but we did not train our network for long... That being said, we have no control of the generated digits. The rest of this task is to explore one solution (a conditional VAE (CVAE)) to generates zeroes, ones, twos and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">The task</font>: conditional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a conditional VAE where you add a categorical variable $c\\in \\{0,\\dots 9\\}$ so that the encoder is now conditioned to two variables X and c: $q_{\\theta}(z|X,c)$ while the decoder is conditioned to the variables z ad c: $p_{\\theta}(X|z,c)$. Now, the real latent variable is distributed under $p(z|c)$.\n",
    "\n",
    "The conditional VAE was proposed by [(Sohn et al., 2015)](https://papers.nips.cc/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html) and is explained in the tutorial [(Doersch, 2016)](https://arxiv.org/abs/1606.05908). \n",
    "\n",
    "<font color=\"red\">Task</font>: Implement a class CVAE and its training by making minimal changes to the VAE code above. You can use the function plot_conditional_generation above to test your code. For instance, plot_conditional_generation(cvae, fix_number=3) should plot 8 images in the same row with independent generations of the number 3.\n",
    "\n",
    "Then move your code to a .py file named run_cvae.py that can be ran directly from a termninal and can take as arguments the number we want to generate. Concretely, we will run the following command in a terminal in a folder in which we have put your file: \n",
    "\n",
    "python run_cvae.py 2\n",
    "\n",
    "This command should save in the same directory a pdf file with the picture of the digits 2 generated by your CVAE (again here, 8 images in the same row with independent generations of the number 2 as in the function above). Of course, we may not use the number 2 but a different number, your file should accomodate that. \n",
    "\n",
    "Your are free to modify the network structure and the training parameters as long as training remains reasonable and you should try to obtain decent digits generation. \n",
    "\n",
    "Please use the same code as above to load the data, it checks if the data is not already present before downloading it. For ecological reasons, we will make sure to run your code in a folder that already contains the data. \n",
    "\n",
    "You should not import any other module than the ones already imported in the code above, with the exception of the sys module to accept argument in the command \"python run_cvae 2\". Make minimal changes to the code above. \n",
    "\n",
    "Note: you will need to add \"n_classes = 10\" as this variables is used in the function \"plot_conditional_generation\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Very (very) important</font>\n",
    "\n",
    "In the run_cvae.py file, you need to include at the beginning (as in previous labs) the following identification: \n",
    "\n",
    "info = {\n",
    "        # TODO replace the following Email with your own\n",
    "        'Email' : 'firstname.lastname@polytechnique.edu',\n",
    "        'Alias' : 'Anonymous', # optional\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "VAE_clustering_empty.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

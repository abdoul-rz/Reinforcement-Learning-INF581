{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24e99bb5",
      "metadata": {
        "id": "24e99bb5"
      },
      "source": [
        "# INF581 - Lab 01\n",
        "\n",
        "### Main Objectives of the Lab\n",
        "\n",
        "Intelligent decision making involves several components. Today we will study, in the context of a toy (low-dimensional, synthetic) example: *perception* (observation), *knowledge* (representation), *reasoning* (inference), and *acting* (decision-making). We will _not_ look at (today) at learning and sequential decision making. Using probabalistic tools covered in the lecture (Bayesian networks, marginalization, ...), the objective is to design a rational/intelligent agent, i.e., an agent that maximizes expected reward."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c31b14d",
      "metadata": {
        "id": "8c31b14d"
      },
      "source": [
        "\n",
        "### Instructions\n",
        "\n",
        "Work your way through the notebook, and provide code to complete the tasks. You can add as much code (and as many code blocks) as you want to solve the task, but make sure it is in a block containing an `## EXTRACT` tag, and don't change the names of existing functions or their parameters. Do not remove or add any of the `## EXTRACT` markers. Check Moodle for details on how to submit work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd4f4717",
      "metadata": {
        "id": "fd4f4717"
      },
      "source": [
        "## Task 0: Name your work\n",
        "\n",
        "Replace the values in the following dictionary `info`. Your Email must match your class email address. Your Alias will be shown on the public leaderboard (to identify yourself)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5e0643a7",
      "metadata": {
        "id": "5e0643a7"
      },
      "outputs": [],
      "source": [
        "## EXTRACT\n",
        "\n",
        "info = {\n",
        "        'Email' : 'abdoul.zeba@polytechnique.edu',\n",
        "        'Alias' : 'Tokic', # (change this in case you want to identify yourself on the leaderboard)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99bf216f",
      "metadata": {
        "id": "99bf216f"
      },
      "source": [
        "### Imports\n",
        "\n",
        "First, we're going to import `numpy` and some utility functions/classes that we will use throughout. You can come back later and add your own code if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f83e88e7",
      "metadata": {
        "id": "f83e88e7"
      },
      "outputs": [],
      "source": [
        "## EXTRACT\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def tile2cell(y, n_col):\n",
        "    ''' utility function: convert tile number (1,...,n) to matrix row, col'''\n",
        "    return int(y - 1) // n_col, int(y - 1) % n_col\n",
        "\n",
        "class PMF:\n",
        "\n",
        "    ''' Probability Mass Function representation and associated functions'''\n",
        "\n",
        "    d = {}\n",
        "\n",
        "    def __init__(self,d = {}):\n",
        "        ''' use dictionary d to represent a pmf (and check that it is normalized) '''\n",
        "        Z = np.sum(np.array(list(d.values())))\n",
        "        if Z != 0 and Z != 1:\n",
        "            # normalize\n",
        "            d = {key: value / Z for key, value in d.items()}\n",
        "\n",
        "        self.d = d\n",
        "\n",
        "    def prob(self, x):\n",
        "        ''' evaluates p(x) where x a numpy array '''\n",
        "        x_str = ' '.join(map(str,x))\n",
        "        if x_str not in self.d.keys():\n",
        "            return 0\n",
        "        return self.d[x_str]\n",
        "\n",
        "    def sample(self):\n",
        "        ''' samples x ~ p(x) where x a numpy array '''\n",
        "        x_str = np.random.choice(list(self.d.keys()), p=np.array(list(self.d.values())))\n",
        "        return np.fromstring(x_str.strip('[]').encode(), sep=' ', dtype=int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2438a23c",
      "metadata": {
        "id": "2438a23c"
      },
      "outputs": [],
      "source": [
        "# Only for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5e5cd6",
      "metadata": {
        "id": "bc5e5cd6"
      },
      "source": [
        "## The Agent\n",
        "\n",
        "Consider an intelligent robot tasked with catching rats in a storage room. It is night time, and the room is dark. You have to rely on auditory information only, but luckily the room is a mess with paper and other debris that means there are distinct sounds which are emitted by a rat as it touches different objects (namely, crinkling and rustling sounds). The room is rectangular, divided up into $n$ square tiles.  A rat has just entered the room (time $t=1$). The agent waits T seconds (until $t=T$), then makes a decision on if and where to pounce (in order to catch the rat).  \n",
        "\n",
        "Let's denote: $y_t \\in \\{1,\\ldots,n\\}$ the position (state) of the rat at time step $t$ (one of $n$ grid tiles); starting at some $y_1$ (entry tile). And $\\mathbf{x}_t \\in \\{0,1\\}^2$ is the 2-dimensional auditory observation at time $t$ (e.g., $\\mathbf{x}_t = [1,0]$ if there is a crinkle but no rustle, etc). The agent accumulates a sequence of $\\mathbf{x}_{1:T} = \\mathbf{x}_1,\\ldots,\\mathbf{x}_T$, which can be considered its input **observation**, with which to make the decision of taking **action** $a$ to pounce (denoting the tile upon which it pounces), or $a=0$ to not pounce. The agent obtains **reward** $r(s,a)$ (this function is already implemented in the environment), obviously catching the rat by pouncing to the correct tile ($s=a$) is better than missing it ($s\\neq a$).\n",
        "\n",
        "Your task is to model this problem, and provide the best action $a$ (according to current knowledge -- a given sequence $\\mathbf{x}_{1:T}$) and associated uncertainty.\n",
        "\n",
        "Be aware of the potential confusion here: $s = y_T$ represents the state of the environment at time $y_T$ and decision making (choosing action $a$) is based on observation $o = \\mathbf{x}_{1:T}$. Since the decision, being made at time $T$, does not affect the future observations, this is a Markov process + a decision rather than a Markov decision process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b051091",
      "metadata": {
        "id": "2b051091"
      },
      "source": [
        "## The Environment\n",
        "\n",
        "You have full *knowledge* of the environment. Indeed, the Environment is fully specified for you, as a Markov process\n",
        "$$\n",
        "    y_{t+1} \\sim p(\\cdot | y_t)\n",
        "$$\n",
        "with observation process $\\mathbf{x}_t \\sim p( \\cdot | y_t)$. A `step` function is implemented to produce the observations.  \n",
        "\n",
        "Simply run the following code block then come back here to read the explanation for what you see.\n",
        "\n",
        "You can inspect the code to see that `P_Y_y` implements $p(\\cdot | y_t)$ such that the target moves by exactly $1$ square tile, either horizontally or vertically (i.e., taxicab-distance) per time step $t$, within the bounds of the of the room, starting at one of the entry points (uniformly at random) as specified by `P_Y`. Insofar as the observation function `P_X_y`: an audible 'crinkle' with probability $\\theta_1$ when over certain tiles (green, or orange) is emitted, and with probability $0$ over other tiles; furthermore, it will invoke a 'rustling' noise with probability $\\theta_2$ over certain tiles (red, or orange), and $0$ otherwise. On orange tiles, both noises are caused independently of each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "179b82f1",
      "metadata": {
        "id": "179b82f1"
      },
      "outputs": [],
      "source": [
        "## EXTRACT ENVIRONMENT\n",
        "\n",
        "class Environment():\n",
        "\n",
        "    def __init__(self, G, theta = [0.9,0.8]):\n",
        "        '''\n",
        "            Environment.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "\n",
        "            G : array_like(int, ndim=2) of shape (n_rows,n_columns)\n",
        "                Specifies a grid where G[j,k] = entry & sound1 & sound2\n",
        "\n",
        "            theta : array_like(float, ndim=1)\n",
        "                Specifies the grid dynamics (acoustics)\n",
        "\n",
        "        '''\n",
        "        # Grid\n",
        "        self.G = G\n",
        "\n",
        "        # Grid shape\n",
        "        self.n_rows = G.shape[0]\n",
        "        self.n_cols = G.shape[1]\n",
        "        self.n_states = self.n_cols * self.n_rows\n",
        "\n",
        "        # State space - tile number representation\n",
        "        self.states = np.arange(1,self.n_states+1)\n",
        "\n",
        "        # Observation function\n",
        "        self.theta = theta\n",
        "\n",
        "    def rwd(self, s, a):\n",
        "        '''\n",
        "            Reward function r(s, a) of taking action a given state s\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            s : int\n",
        "                true state (tile which containts the object)\n",
        "            a : int\n",
        "                estimated state\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            float\n",
        "                reward obtained from taking action a given state s\n",
        "        '''\n",
        "        return (s==a)\n",
        "\n",
        "    def P_X_y(self,y):\n",
        "        ''' Observation distribution.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            s : int\n",
        "                current tile/state\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "                p where p[j] = p(Xj = 1 | y)\n",
        "        '''\n",
        "\n",
        "        # Convert tile-number (y) to row/column representation (i,j)\n",
        "        i,j = tile2cell(y, self.n_cols)\n",
        "\n",
        "        d = {}\n",
        "\n",
        "        if self.G[i,j] == 0 or self.G[i,j] > 3:\n",
        "            # nothing is heard on empty tiles or entry tiles\n",
        "            d = {\n",
        "                '0 0' : 1.0\n",
        "            }\n",
        "        elif self.G[i,j] == 1: # 0,1\n",
        "            # sound '1' is heard with probability theta_1\n",
        "            d = {\n",
        "                '0 1' : self.theta[1],\n",
        "                '0 0' : 1 - self.theta[1],\n",
        "            }\n",
        "        elif self.G[i,j] == 2: # 1,0\n",
        "            # sound '2' is heard with probability theta_0\n",
        "            d = {\n",
        "                '1 0' : self.theta[0],\n",
        "                '0 0' : 1-self.theta[0],\n",
        "            }\n",
        "        elif self.G[i,j] == 3:\n",
        "            # sounds are heard independently: p(x1,x2) = p(x1) * p(x2)\n",
        "            d = {\n",
        "                '0 0' : (1 - self.theta[0]) * (1 - self.theta[1]),\n",
        "                '0 1' : (1 - self.theta[0]) * self.theta[1],\n",
        "                '1 0' : self.theta[0] * (1-self.theta[1]),\n",
        "                '1 1' : self.theta[0] * self.theta[1],\n",
        "            }\n",
        "\n",
        "        return PMF(d)\n",
        "\n",
        "    def P_Y(self):\n",
        "        ''' Distribution P(Y_1).\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            array_like (n_cols * n_rows)\n",
        "                where P[i,j] = P(Y_1 = y) where y the tile at row i, col j\n",
        "        '''\n",
        "        n_entries = np.sum(self.G >= 4)\n",
        "        return (self.G >= 4) * 1 / n_entries\n",
        "\n",
        "    def P_Y_y(self,_y):\n",
        "        ''' Distribution P(Y_t | Y_{t-1}).\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            _y : int\n",
        "                previous tile/state\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            array_like (n_cols * n_rows)\n",
        "                where P[i,j] = P(Y_t = y | Y_{t-1} = _y) where y the tile at row i, col j\n",
        "        '''\n",
        "        if _y is None:\n",
        "            return self.P_Y()\n",
        "\n",
        "        if _y not in self.states:\n",
        "            raise Exception(\"Sorry, %d is not one of the states\" % _y)\n",
        "\n",
        "        j,k = tile2cell(_y, self.n_cols)\n",
        "\n",
        "        G = np.zeros_like(self.G)\n",
        "        if j > 0:\n",
        "            G[j-1,k] = 1\n",
        "        else:\n",
        "            G[j,k] = 1\n",
        "        if j < (self.n_rows-1):\n",
        "            G[j+1,k] = 1\n",
        "        else:\n",
        "            G[j,k] = 1\n",
        "        if k > 0:\n",
        "            G[j,k-1] = 1\n",
        "        else:\n",
        "            G[j,k] = 1\n",
        "        if k < (self.n_cols-1):\n",
        "            G[j,k+1] = 1\n",
        "        else:\n",
        "            G[j,k] = 1\n",
        "\n",
        "        return G / np.sum(G)\n",
        "\n",
        "    def step(self, _y):\n",
        "        ''' Step to the next state, given current state _y.\n",
        "\n",
        "            The agent's actions do not affect the environment.\n",
        "\n",
        "\n",
        "            Paramaters\n",
        "            ----------\n",
        "\n",
        "            _y : int\n",
        "                current state\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "\n",
        "            y : int\n",
        "                next state\n",
        "            o : int\n",
        "                corresponding observation\n",
        "        '''\n",
        "        # Generate a state y' ~ p( . | y)\n",
        "        w = self.P_Y_y(_y).flatten()\n",
        "        y = np.random.choice(self.n_states,p=w) + 1\n",
        "\n",
        "        # Generate an observation x' ~ p(. | y')\n",
        "        pmf = self.P_X_y(y)\n",
        "        x = pmf.sample()\n",
        "\n",
        "        return y, x\n",
        "\n",
        "\n",
        "    def gen_path(self, T=5):\n",
        "        ''' Generate a path with associated observations.\n",
        "\n",
        "\n",
        "            Paramaters\n",
        "            ----------\n",
        "\n",
        "            T : int\n",
        "                how long is the path\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "\n",
        "            x : (T,d)-shape array\n",
        "                sequence of observations\n",
        "            y : T-length array of states\n",
        "                sequence of tiles\n",
        "        '''\n",
        "\n",
        "        x = np.zeros((T,len(self.theta)),dtype=int)\n",
        "        y = np.zeros(T,dtype=int)\n",
        "\n",
        "        # (t-1)-th state\n",
        "        _y = None\n",
        "\n",
        "        for t in range(T):\n",
        "\n",
        "            # Generate next step\n",
        "            y[t], x[t] = self.step(_y)\n",
        "\n",
        "            # And remember this state\n",
        "            _y = y[t]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def plot_scenario(self, y_seq=None, x_seq=None, dgrid=None, a_star=None, paths=[], title=None):\n",
        "        '''\n",
        "            Plot a visual representation of the environment.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "\n",
        "            y_seq : numpy array (dtype=int)\n",
        "                a path (e.g., [1,3,1,2])\n",
        "\n",
        "            x_seq :\n",
        "                observations associated with the path\n",
        "\n",
        "            dgrid : shape like self.G\n",
        "                contains values (e.g., probabilities) to show in each tile\n",
        "\n",
        "            a_star : int\n",
        "                the optimal action\n",
        "\n",
        "            title : str\n",
        "                a title for the plot\n",
        "\n",
        "        '''\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=[8,4])\n",
        "\n",
        "        colors = {\n",
        "            0 : \"white\",    # nothing\n",
        "            1 : \"green\",    # sound 1\n",
        "            2 : \"red\",      # sound 2\n",
        "            3 : \"orange\",   # sound 1 + 2\n",
        "            4 : \"yellow\"    # entry\n",
        "        }\n",
        "        labels = ['', 'Crinkle', 'Rustle', 'Crinkle/rustle', 'Entry']\n",
        "\n",
        "        # Plot the tiles in the room ...\n",
        "\n",
        "        if dgrid is None:\n",
        "            # ... as a visual representation\n",
        "            im = ax.imshow(self.G, cmap=ListedColormap(list(colors.values())), alpha=0.3)\n",
        "            patches = [mpatches.Patch(color=colors[i], alpha=0.3, label=labels[i]) for i in [1,2,3,4]]\n",
        "            plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
        "        else:\n",
        "            # ... as a probability mass function\n",
        "            im = ax.imshow(dgrid, cmap=plt.cm.Reds)\n",
        "\n",
        "        # Plot the path, alongside the observations generated by that path, and the optimal action.\n",
        "\n",
        "        if y_seq is not None:\n",
        "\n",
        "            # Draw the path\n",
        "            T = len(y_seq)\n",
        "            y_coords = np.array([tile2cell(y_t,env.n_cols)[0] for y_t in y_seq]) + np.random.randn(T)*0.1\n",
        "            x_coords = np.array([tile2cell(y_t,env.n_cols)[1] for y_t in y_seq]) + np.random.randn(T)*0.1\n",
        "            ax.plot(x_coords,y_coords,\"ko-\")\n",
        "            ax.plot(x_coords[-1],y_coords[-1],\"kx\",markersize=20)\n",
        "\n",
        "            # Draw the action (i.e., target tile)\n",
        "            if a_star is not None:\n",
        "                y_coord = tile2cell(a_star,env.n_cols)[0]\n",
        "                x_coord = tile2cell(a_star,env.n_cols)[1]\n",
        "                ax.plot(x_coord,y_coord,\"m+\",markersize=15)\n",
        "\n",
        "            # Draw the sounds (observations)\n",
        "            if x_seq is not None:\n",
        "                ax.scatter(np.array(x_coords)[x_seq[:, 0] > 0], np.array(y_coords)[x_seq[:, 0] > 0], marker='o', s=200, facecolors='none', linewidths=3, edgecolors=colors[2])\n",
        "                ax.scatter(np.array(x_coords)[x_seq[:, 1] > 0], np.array(y_coords)[x_seq[:, 1] > 0], marker='o', s=400, facecolors='none', linewidths=3, edgecolors=colors[1])\n",
        "\n",
        "\n",
        "        for path in paths:\n",
        "            # Draw the path\n",
        "            T = len(path)\n",
        "            y_coords = np.array([tile2cell(s,env.n_cols)[0] for s in path]) + np.random.randn(T)*0.1\n",
        "            x_coords = np.array([tile2cell(s,env.n_cols)[1] for s in path]) + np.random.randn(T)*0.1\n",
        "            ax.plot(x_coords,y_coords,\"mo:\")\n",
        "            ax.plot(x_coords[-1],y_coords[-1],\"mx\",markersize=10)\n",
        "\n",
        "\n",
        "        # Ticks and grid\n",
        "\n",
        "        ax.set_xticks(np.arange(0, self.n_cols, 1))\n",
        "        ax.set_xticks(np.arange(-0.5, self.n_cols, 1), minor=True)\n",
        "        ax.set_xticklabels(np.arange(0, self.n_cols, 1))\n",
        "\n",
        "        ax.set_yticks(np.arange(0, self.n_rows, 1))\n",
        "        ax.set_yticks(np.arange(-0.5, self.n_rows, 1), minor=True)\n",
        "        ax.set_yticklabels(np.arange(0, self.n_rows, 1))\n",
        "\n",
        "        ax.grid(which='minor', color='k')\n",
        "\n",
        "        n = 0\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(self.n_cols):\n",
        "              ax.text(j, i, self.states[n], va='center', ha='center')\n",
        "              n = n + 1\n",
        "\n",
        "\n",
        "        # Title\n",
        "\n",
        "        if title is not None:\n",
        "            ax.set_title(title)\n",
        "\n",
        "\n",
        "        # Return\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig, ax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caff2102",
      "metadata": {
        "id": "caff2102"
      },
      "source": [
        "### Instantiating the Environment\n",
        "\n",
        "Let's instantiate an environment, generate a path, and plot it. It is important to realise that although the agent can have full access to the environment, as well as observations, we do not have access to the true path $y_1,\\ldots,y_T$ and hence the challenge in estimating $y_T$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8b82c9d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "8b82c9d6",
        "outputId": "8ae21ffb-d22b-415b-c584-23a1fbe04e02"
      },
      "outputs": [],
      "source": [
        "# Design a map\n",
        "G = np.array([[1,3,0,2,4,1],\n",
        "              [2,1,0,3,0,3],\n",
        "              [4,0,3,0,2,0],\n",
        "              [3,1,2,3,0,4],\n",
        "              [2,0,0,0,1,1]]);\n",
        "\n",
        "# Init. the environment\n",
        "env = Environment(G)\n",
        "\n",
        "# Generate a path\n",
        "xxx, inconstruct_path = env.gen_path()\n",
        "fig, ax = env.plot_scenario(y_seq=inconstruct_path, x_seq=xxx, title=\"$y_{1:T}$ = %s\" % (str(inconstruct_path)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d17644d",
      "metadata": {
        "id": "1d17644d"
      },
      "source": [
        "### Implementing the Agent\n",
        "\n",
        "Recall: The Agent is responsible for receiving observation $o = \\mathbf{x}_{1:T}$ and producing prediction $a$, i.e., it implements $a = \\pi(o)$, i.e., its policy or `action_selection` function as it is called here below.\n",
        "\n",
        "The following tasks should be implemented within the following `Agent` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9a7ad571",
      "metadata": {
        "id": "9a7ad571",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "## EXTRACT AGENT\n",
        "\n",
        "class Agent():\n",
        "\n",
        "    # TODO (optional): Add any auxilliary functions you might use here\n",
        "\n",
        "    def P_YYY_xxx(self,xxx,env):\n",
        "        '''\n",
        "        Full conditional distribution of a path given sequence of observations.\n",
        "\n",
        "        $$\n",
        "            P( Y_1,...,Y_T | x_1,...,x_T )\n",
        "        $$\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        xxx : array_like(int,ndim=2)\n",
        "            T observations (of 2 bits each)\n",
        "\n",
        "        env : Environment\n",
        "            the environment that produced observation x\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict(str:float)\n",
        "            d such that d[str(yyy)] is the probability of observing path yyy\n",
        "            and if str(y) not in d, implies 0 probability\n",
        "            where yyy is array_like(int, ndim=1), e.g., [5, 4, 7, 13, 12]\n",
        "        '''\n",
        "        d = {}\n",
        "        # TODO Task 1\n",
        "        if len(xxx) == 0:\n",
        "            return d\n",
        "        for y in env.states:\n",
        "            i, j = tile2cell(y, env.n_cols)\n",
        "            prob = env.P_Y()[i, j] * env.P_X_y(y).prob(xxx[0])\n",
        "            if (prob > 0):\n",
        "                paths = self.deep_search(env, xxx, [y], prob, len(xxx) - 1)\n",
        "                for path, p in paths:\n",
        "                    path = ' '.join(map(str, path))\n",
        "                    d[path] = d.get(str(path), 0) + p        \n",
        "        return PMF(d)\n",
        "    \n",
        "        \n",
        "    def deep_search(self, env, xxx, inconstruct_path, p, t_inv):\n",
        "        '''\n",
        "        return all possible paths with probability p\n",
        "        '''\n",
        "        result = []\n",
        "        if (t_inv == 0):\n",
        "            return [[inconstruct_path, p]]\n",
        "        len_y = len(inconstruct_path)\n",
        "        last_y, last_x = inconstruct_path[-1], xxx[len_y]\n",
        "        for y in env.states:\n",
        "            i, j = tile2cell(y, env.n_cols)\n",
        "            prob = p * env.P_Y_y(last_y)[i, j] * env.P_X_y(y).prob(last_x)\n",
        "            if (prob > 0):\n",
        "                current_path = inconstruct_path + [y]\n",
        "                next_path = self.deep_search(env, xxx, current_path, prob, t_inv - 1)\n",
        "                result.extend(next_path)\n",
        "        return result\n",
        "\n",
        "    def P_Y_xxx(self,xxx,env,t=-1):\n",
        "        '''\n",
        "        The (conditional) marginal distribution on state t under observation xxx.\n",
        "\n",
        "        $$\n",
        "            P(Y_t | x_1,...,x_T ).\n",
        "        $$\n",
        "\n",
        "        The probability (distribution) of the t-th state, given all the observed evidence.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        xxx : array_like(int,ndim=2)\n",
        "            T observations (of 2 bits each)\n",
        "\n",
        "        t : int\n",
        "            the given state, e.g., 3, or -1 for final state\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        P : array_like(float,ndim=2)\n",
        "            such that P[i,j] is the probability of being in state y_t at time t\n",
        "            where i,j = tile2cell(y_t)\n",
        "        '''\n",
        "\n",
        "        d_marg = {}\n",
        "\n",
        "        pmf = self.P_YYY_xxx(xxx,env) # Hint: this can save you some work\n",
        "\n",
        "        P = np.zeros_like(env.G,dtype=float)\n",
        "\n",
        "        # TODO Task 2\n",
        "\n",
        "        for yyy, p in pmf.d.items():\n",
        "            split  = yyy.split(' ')\n",
        "            y_t = split[t]\n",
        "            d_marg[y_t] = d_marg.get(y_t,0) + p\n",
        "\n",
        "        for y_t, p in d_marg.items():\n",
        "            i,j = tile2cell(int(y_t), env.n_cols)\n",
        "            P[i,j] += p\n",
        "        return P\n",
        "\n",
        "    def Q_A(self,xxx,env):\n",
        "        '''\n",
        "        We want the reward (value) for any given action a.\n",
        "\n",
        "        $$\n",
        "            E[ r(S,a) | x_1,...,x_T ]\n",
        "        $$\n",
        "\n",
        "        Since you do not know the exact state, you need to marginalise out your uncertainty.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        xxx : array_like(int,ndim=2)\n",
        "            T observations (of 2 bits each)\n",
        "\n",
        "        env : Environment\n",
        "            the environment that implements env.rwd(s,a)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        Q : array_like(float,ndims=2)\n",
        "            such that Q[i,j] where tile2cell(a) is the value (expected reward) of action a.\n",
        "\n",
        "        '''\n",
        "        Q = np.zeros_like(env.G,dtype=float)\n",
        "        P = self.P_Y_xxx(xxx,env) # Hint\n",
        "        # TODO Task 3\n",
        "        for a in env.states:\n",
        "            i,j = tile2cell(a,env.n_cols)\n",
        "            Q[i,j] = self.cond_esp_rwd(a, P, env)\n",
        "        return Q\n",
        "\n",
        "    def cond_esp_rwd(self, a, P, env):\n",
        "        '''\n",
        "        The conditional expectation of the reward given action a and observation xxx.\n",
        "\n",
        "        $$\n",
        "            E[ r(S,a) | x_1,...,x_T ]\n",
        "        $$\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        a : int\n",
        "            the action\n",
        "\n",
        "        xxx : array_like(int,ndim=2)\n",
        "            T observations (of 2 bits each)\n",
        "\n",
        "        env : Environment\n",
        "            the environment that implements env.rwd(s,a)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        float\n",
        "            the expected reward\n",
        "\n",
        "        '''\n",
        "        r = 0\n",
        "        for s in env.states:\n",
        "            i,j = tile2cell(s,env.n_cols)\n",
        "            r += env.rwd(s,a) * P[i,j]\n",
        "        return r\n",
        "\n",
        "    def action_selection(self,xxx,env):\n",
        "        '''\n",
        "        Decide on the best action to take, under observation xxx.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        xxx : array_like(int,ndim=2)\n",
        "            T observations (of 2 bits each)\n",
        "\n",
        "        env : Environment\n",
        "            the environment that implements env.rwd(s,a)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        int\n",
        "            the chosen action a\n",
        "\n",
        "        '''\n",
        "        Q = self.Q_A(xxx,env) # Hint\n",
        "        # TODO Task 3\n",
        "        Q = Q.flatten()\n",
        "        a = np.argmax(Q) + 1\n",
        "        return a\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "5fb1488d",
      "metadata": {
        "id": "5fb1488d"
      },
      "outputs": [],
      "source": [
        "# Init. the agent\n",
        "agent = Agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c087a62f",
      "metadata": {
        "id": "c087a62f"
      },
      "source": [
        "## Task 1: Joint distribution (distribution over paths)\n",
        "\n",
        "What is the distribution over all paths the target could have taken for a given sequence of observations? We are looking for\n",
        "$$\n",
        "    P(Y_{1:T} | \\mathbf{x}_{1:T})\n",
        "$$\n",
        "\n",
        "**Task**: Edit the `Agent` code where indicated by `TODO`; specifically relating to function `P_YYY_xxx` (but you may add other functions if you need them). Run the following cell to have an idea of what is expected.  \n",
        "\n",
        "**Note**: Your agent should operate on the environment `env` supplied to it. You should *not* assume global access to the `env` instantiated above.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "be08a60f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "be08a60f",
        "outputId": "112404d1-c8b3-4002-f31b-ccdcc17f7203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We could expect the path [24 30 29 23 22] with probabilty 0.039353 \n"
          ]
        }
      ],
      "source": [
        "# Task 1. Get joint-conditional distribution\n",
        "pmf = agent.P_YYY_xxx(xxx,env)\n",
        "path = pmf.sample()\n",
        "print(\"We could expect the path %s with probabilty %8.6f \" % (path, pmf.prob(path)))\n",
        "\n",
        "# Generate some possible paths\n",
        "paths = [pmf.sample() for i in range(10)]\n",
        "\n",
        "# ... and plot them\n",
        "fig, ax = env.plot_scenario(y_seq=inconstruct_path, x_seq=xxx, paths=paths, title=\"A sample of likely paths\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4f9bee",
      "metadata": {
        "id": "bb4f9bee"
      },
      "source": [
        "## Task 2: Marginal distribution (over final states)\n",
        "\n",
        "What is the distribution over all final states the target could be at? We are looking for\n",
        "$$\n",
        "    P(Y_{T} | \\vec{x}_{1:T})\n",
        "$$\n",
        "\n",
        "**Task**: Edit the `Agent` code where indicated by `TODO`, in particular the `P_Y_xxx` function. Check the plot produced by the next cell to have an idea of what is expected. Recall: we do not have access to the path shown, only observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4b3b3875",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "4b3b3875",
        "outputId": "35b304bf-ff7f-46e9-b50c-baeecb6b0ae1"
      },
      "outputs": [],
      "source": [
        "# Task 2. Get marginal distribution\n",
        "P = agent.P_Y_xxx(xxx,env)\n",
        "fig, ax = env.plot_scenario(y_seq=inconstruct_path, x_seq=xxx, dgrid=P, title=\"Marginal distribution of final states\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3daedb1e",
      "metadata": {
        "id": "3daedb1e"
      },
      "source": [
        "## Task 3: Taking an Action\n",
        "\n",
        "You've now observed evidence $\\mathbf{x}_1,\\ldots,\\mathbf{x}_T$, and queried the model, according to your beliefs (environment dynamics). Time to make a decision. Which action to take?\n",
        "\n",
        "$$\n",
        "    a_* = \\text{argmax}_a E_{S \\sim P(Y_T | \\mathbf{x}_{1:T})}[ r(S, a) ]\n",
        "$$\n",
        "\n",
        "Note your uncertainty about the final state $S$.\n",
        "\n",
        "In this scenario the action will not affect future observations (because $y_T$ is the final observation), thus you are essentially making an estimate:\n",
        "$$\n",
        "    a = \\hat y_{T} = \\pi(\\mathbf{x}_{1:T})\n",
        "$$\n",
        "\n",
        "**Task 3a**: You need to evaluate the expectation. As this is essentially 'value', we call it `Q` inline with reinforcement learning terminology. You complete this task in the function `Q_A`. Hint: You already have $p(Y_T | \\vec{x}_{1,\\ldots,T})$\n",
        "\n",
        "**Task 3b**: Wrap the argmax around the outside (i.e., make the decision/take the action). This involves the function `action_selection`.\n",
        "\n",
        "Run the following code block to have a look at the outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "14232bfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "14232bfe",
        "outputId": "90a230b5-4b9d-4b66-f476-5f9f1d2b3eb0"
      },
      "outputs": [],
      "source": [
        "# Task 3. Obtain values\n",
        "Q = agent.Q_A(xxx,env)\n",
        "\n",
        "# Decide on an action\n",
        "a = agent.action_selection(xxx, env)\n",
        "\n",
        "# Compare to the ground truth\n",
        "r = env.rwd(inconstruct_path[-1],a)\n",
        "\n",
        "# Plot the result\n",
        "fig, ax = env.plot_scenario(y_seq=inconstruct_path, dgrid=Q, a_star=a, x_seq=xxx, title=\"Rewards for action %d; $r(%d,%d) = %d$\" % (a,inconstruct_path[-1],a,r))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aca0e2d",
      "metadata": {
        "id": "3aca0e2d"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Did you get the best reward? Remember, an agent is not expected to obtain maximum reward, it is expected to obtain (close to) maximum expected reward, under the uncertainty implied by the environment. The following code block will help ascertain the 'success rate' of your agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "e02ee81e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e02ee81e",
        "outputId": "f2538d7c-46ca-4015-e84f-b7ff467420f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4400000000000002\n"
          ]
        }
      ],
      "source": [
        "env = Environment(G)\n",
        "\n",
        "# Check average performace over n trials\n",
        "n = 100\n",
        "r_avg = 0\n",
        "for i in range(n):\n",
        "    xxx, inconstruct_path = env.gen_path()\n",
        "    a = agent.action_selection(xxx, env)\n",
        "    r_avg += env.rwd(inconstruct_path[-1],a) / n\n",
        "print(r_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2237f94d",
      "metadata": {
        "id": "2237f94d"
      },
      "source": [
        "## Conclusion (So What?)\n",
        "\n",
        "Was it the right action? Did you expect to be? Recall what it implies (and what it doesn't imply) to be an optimal agent.\n",
        "\n",
        "This was just a toy example, but consider the fundamental concepts here (we will be using them again); and think about real-world examples where such an approach might be relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a2caeb6",
      "metadata": {
        "id": "4a2caeb6"
      },
      "source": [
        "It was not the right action every time and I expect it since we maximize the expected reward and not the reward itself. But it is the right action most of the time. With the uncertainty of the environment, it is not possible to maximize directly the reward. So an optimal agent may imply to take actions to maximize the expected cumulative reward and not the reward itself.\n",
        "\n",
        "In a real-world scenario, this approach might be relevant in situations where the environment is uncertain, and the agent needs to make decisions based on partial information.\n",
        "\n",
        "#### Environment\n",
        "- The robot operates in a city environment with various obstacles, pedestrians, and unpredictable traffic conditions.\n",
        "- The robot's goal is to reach a destination specified by a user.\n",
        "\n",
        "#### Agent\n",
        "- The delivery robot equipped with sensors and cameras for perception.\n",
        "- The robot's decision-making process involves selecting actions such as turning, stopping, or changing lanes.\n",
        "\n",
        "#### Uncertainty\n",
        "- The environment is dynamic and can change rapidly. Pedestrians may cross the road unexpectedly, vehicles may change lanes, and traffic conditions can vary.\n",
        "- The robot's sensors provide partial and noisy information about the surroundings."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
